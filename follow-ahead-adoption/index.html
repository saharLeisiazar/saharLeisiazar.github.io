<html lang="en">
<head>
    <title>Follow-ahead-adoption</title>
    <meta name="description" content="Project page for Adapting to Frequent Human Direction Changes in Autonomous Frontal Following Robots.">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <meta charset="utf-8">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="style.css" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>    
</head>
<body>


<!--Title-->    
<div class="container" style="text-align:center; padding:2rem 15px">
    <div class="row" style="text-align:center">
        <h1>Adapting to Frequent Human Direction Changes in Autonomous Frontal Following Robots</h1>
        <h4>IEEE Robotics and Automation Letters 2024</h4>
    </div>
    <div class="row" style="text-align:center">
        <div class="col-xs-0 col-md-3"></div>
        <div class="col-xs-12 col-md-6">
        <h4>
    
            <!-- How to write authors and how to do we need links?-->
            <nobr>Sahar Leisiazar</nobr><sup></sup> &emsp;
            <nobr>Seyed Roozbeh Razavi Rohani</nobr><sup></sup> &emsp;
            <nobr>Edward J. Park</nobr><sup></sup> &emsp;
            <nobr>Angelica Lim</nobr><sup></sup> </a>&emsp;
            <nobr>Mo Chen</nobr><sup></sup></a>&emsp;

        </h4>


        <p><em><center>School of Mechatronics System Engineering, Simon Fraser University, Surrey, BC, Canada  &emsp;</center></em></p>
        <p><em><center>School of Computing Science, Simon Fraser University, Burnaby, BC, Canada  &emsp;</center></em></p>
        </div>
        <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
        <a href="" style="color:inherit">
            <i class="fa fa-file-pdf-o fa-4x"></i></a> 
        </div>
        <!-- <div class="hidden-xs hidden-sm col-md-2" style="text-align:left; margin-left:0px;">
        <a href="https://github.com/monniert/dti-clustering" style="color:inherit">
            <i class="fa fa-github fa-4x"></i></a>
        </div> -->
    </div>
</div>





<!--Links-->
<div class="container" style="text-align:center; padding:1rem">
    <h3 style="text-align:center; padding-top:1rem; padding-bottom: 2em">
      <!--<a class="label label-info" href="resrc/paper.pdf">Paper</a>
      <a class="label label-info" href="resrc/annotate_no_gpt.csv">Annotations</a>
      <a class="label label-info" href="resrc/annotate_gpt.csv">Annotations  w/Results</a>
      <a class="label label-info" href="https://dev9032.d1ab6be83l9u2m.amplifyapp.com/">Annotation Website</a>
      <a class="label label-info" href="resrc/ref.bib">BibTeX</a> -->
      <a class="label label-info" href="resrc/arxiv-followahead.pdf">Paper</a>
      <a class="label label-info" href="https://github.com/saharLeisiazar">Code(will be publish after acceptance)</a>
      <a class="label label-info" href="https://www.youtube.com/watch?v=sPF8eJbNYIU&list=PLMSLU9mHe0DlH-nXzEEjmA35DtHhZh5GL&index=2">Video</a>
<!--       <a class="label label-info" href="resrc/IROS2023-MCTS-DRL.pdf">Presentation</a>
      <a class="label label-info" href="resrc/IROS2023_Poster.pdf">Poster</a> -->
    </h3>

    <div class="row" style="text-align:center; padding: 1rem">
          <div class="col-sm-6">
              <img src="resrc/first.drawio.png" alt="dti.png"  class="text-center" style="width: 100%; max-width: 1100px">
            </div>
            <div class="col-sm-6">
              <img src="resrc/tree_expansion.drawio.png" alt="deep_tsf.png"  class="text-center" style="width: 60%; max-width: 1100px">
            </div>
    </div>

      <div class="row" style="text-align:center; padding:1rem">
            <div class="col-sm-6">
              <p><h5><center>Real-world experiment showcasing the robot's ability to follow a human from the front while avoiding obstacles. The figure illustrates an example of the tree expansion process, with blue and red arrows representing the potential moves for the robot and human, respectively. The robot expands the tree and selects nodes that allow it to maintain a position in front of the human, while eliminating branches that could lead to collisions with obstacles.</center></h5></p>
            </div>
            <div class="col-sm-6">
              <p><h5><center>Tree expansion in MCTS: Blue nodes represent possible future positions of the robot, while red nodes indicate potential future positions of the human. The letters "L", "R", "S", and "F" on the edges of the tree represent the actions: Left, Right, Straight, and Fast, respectively.</center></h5></p>
            </div>
      </div>

    
<!--     <img src="resrc/first.drawio.png" alt="teaser.png" class="text-center" style="width: 100%; max-width: 2000px"> -->
<!--     <p><h5><em>llustration of the MCTS-DRL framework utilizing
        human future estimation for goal generation. The search
        tree is expanded to identify the best goal point to follow
        ahead of the person, while avoiding occlusion and collision.
        The resulting goal point is indicated by a green star, while
        red and blue arrows represent paths leading to collision and
        occlusion, respectively. The MCTS algorithm expands a tree
        to find the best navigational goal for the robot in order to
        follow-ahead of the target person and avoid collision and
        occlusion caused by surrounding objects.</h5></p>    
</div> -->



<div class="container">
    <h3>Abstract</h3>
    <hr/>
    <p>
        This paper addresses the challenge of robot follow ahead applications where the human behavior is highly variable. We propose a novel approach that does not rely on single human trajectory prediction but instead considers multiple potential future positions of the human, along with their associated probabilities, in the robot’s decision-making process. We trained an LSTM-based model to generate a probability distribution over the human’s future actions. These probabilities, along with different potential actions and future positions, are integrated into the tree expansion of Monte Carlo Tree Search (MCTS). Additionally, a trained Reinforcement Learning (RL) model is used to evaluate the nodes within the tree. 
By incorporating the likelihood of each possible human action and using the RL model to assess the value of the different trajectories, our approach enables the robot to effectively balance between focusing on the most probable future trajectory and considering all potential trajectories. This methodology enhances the robot's ability to adapt to frequent and unpredictable changes in human direction, improving its navigation and ability to navigate in front of the person.
    </p>



    <h3>Approach</h3>
    <hr/>
    
    <p>In this paper, we present an algorithm designed for a mobile robot to follow a human while accounting for the dynamic nature of human movement, rather than assuming that the human has a fixed goal point. Given that humans frequently change direction, the algorithm considers a range of possible future actions for both the robot and the human.
The robot is designed to evaluate the most promising and probable moves and future positions of both the human and itself a few time steps ahead, ultimately selecting the optimal action for the current time step.

The algorithm is structured into three key modules: 
    (i) Decision tree: A module that takes the poses of both the human and the robot as inputs. It expands the tree by considering various potential actions for both the robot and the human, ultimately generating the optimal action for the robot.
    (ii) State evaluation: A module that assigns a value to each node during tree expansion. Nodes with higher values indicate that the robot and human are in a more desirable relative position.
    (iii) Human future positioning probabilities: This module estimates the likelihood of different future human positions for each human node during tree expansion. This helps the algorithm to better anticipate and adapt to realistic human behavior in real-world scenarios.
</p>
    
      <div class="row" style="text-align:center; padding:1rem">
        <div class="col-sm-6">
          <img src="resrc/algorithm.png" alt="dti.png"  class="text-center" style="width: 85%; max-width: 1100px">
        </div>
        <div class="col-sm-6">

          <p>The proposed approach combines Monte Carlo Tree Search (MCTS) and Deep Reinforcement Learning (DRL) 
            to efficiently explore and evaluate possible trajectories. MCTS is used to expand a decision tree 
            considering robot and human poses, while DRL aids in node evaluation. The algorithm generates 
            navigational goals while considering obstacle avoidance. It assigns values to nodes based on occlusions 
            and collisions and uses the Upper Confidence Bound (UCB) to select the best node as the robot's navigational goal. 
            This process repeats iteratively, resulting in the robot's optimal path for the next three seconds.</p>
            <p>We used DDQN to train, in an obstacle-free environment, 
              to obtain a Q function, which takes as input the
              observation and actions, and produces as output the estimated expected
              return (discounted sum of reward). During the tree expansion of MCTS, the trained Q function is
              utilized to estimate the expected reward of each node instead of random sampling. The reward 
              function is determined by considering the robot's relative distance and orientation:   </p>

              <img src="resrc/r.png" alt="dti.png"  class="text-center" style="width: 80%; max-width: 1100px">              
        </div>
      </div>    
    



    <h3>Experiments</h3>
    <hr/>
      <!-- first experiment -->
      <div class="container" style="text-align:center; padding:1rem">

        <pre><p><h4><em>MCTS-DRL vs MCTS and DRL</h4></p></pre>
        <p style="padding-top: 1em; padding-bottom: 2em">
        The experiment is conducted in two distinct human trajectories: a circular path and an S-
        shaped path. The results reveal that the DRL approach is
        unable to effectively follow the human and avoid obstacles,
        which is attributed to the training of the model in an
        obstacle-free environment. Also, the MCTS approach fails to generate
        consistent results, particularly around corners which is due
        to its random action selection. In contrast, the proposed
        MCTS-DRL method demonstrates a superior performance,
        generating a trajectory for the robot that effectively maintains
        a specified distance from the human, avoids occlusion and
        collisions with obstacles, and results in a consistent and
        stable behavior.</p>
         
        <img src="resrc/exp1.png" alt="teaser.png" class="text-center" style="width: 65%; max-width: 20000px">
        <p><h5><em>Performance comparison of DRL, MCTS, MCTS-DRL for two different human trajectories. The human and robot
          trajectories are depicted by a line and points, respectively. The rainbow color scale indicates the time dimension, with purple
          and red denoting the first and last time-steps, respectively. The MCTS-DRL method outperforms the standalone MCTS and
          DRL algorithms, effectively following in front of the human, avoiding obstacles, and producing a consistent and stable
          behavior.</h5></p>    
      </div>


      <!-- second experiment -->
      <div class="container" style="text-align:center; padding:1rem">
        <pre><p><h4>Follow-ahead in an obstacle-free environment</u></h4></p></pre> 
        <p style="padding-top: 1em; padding-bottom: 2em">
          We conducted a comparison between the performance
          of human following in an obstacle-free environment using
          the proposed MCTS-DRL method and the LBGP method
          proposed in [1]. The outcomes of the experiments, in
          terms of the mean human-robot relative distance and mean
          orientation angle (α), are presented in the following table. The results demonstrated that the robot successfully main-
          tained the human-robot relative distance within the range
          of [1, 2] m for all trajectories, and attempted to follow the
          human in front. This experiment demonstrates the efficacy of
          the proposed MCTS-DRL approach in achieving comparable
          results to previous methods in obstacle-free environments.</p>
        <p>[1] P. Nikdel, R. Vaughan, and M. Chen, “Lbgp: Learning based goal
        planning for autonomous following in front,” in 2021 IEEE Interna-
        tional Conference on Robotics and Automation (ICRA). IEEE, 2021,
        pp. 3140–3146.</p>


        <p><h5 style="padding-top:2em "><em>Follow-ahead comparative results for three human trajectories in an obstacle free environment. The proximity of
          the distance to 1.5 and the orientation to 0 indicates better performance.</h5></p> 
        <img src="resrc/exp2.png" alt="teaser.png" class="text-center" style="width: 85%; max-width: 2000px">
      </div>


      <!-- third experiment -->
      <div class="row" style="text-align:center; padding:3rem">
      <pre><p><h4><center><em>Obstacle and occlusion avoidance</center></h4></p></pre>
      </div>
      <p style="padding-top: 1em; padding-bottom: 2em"> Since there are no existing follow-ahead methods proposed
        for environments with obstacles, comparisons with other
        methods were not possible. Therefore, the following scenarios 
        were conducted to evaluate the performance of the proposed method in the presence and absence of obstacles within the environment. Our algorithm is capable of following the
        target person along any random trajectory. </p>

      <img src="resrc/mcts_long.gif" alt="teaser.png" class="text-center" style="width: 100%; max-width: 2000px">

      <div class="row" style="text-align:center; padding:1rem">
          <div class="col-sm-6">
            <img src="resrc/exp3-straight.png" alt="dti.png"  class="text-center" style="width: 100%; max-width: 1100px">
          </div>
          <div class="col-sm-6">
            <img src="resrc/exp3-U.png" alt="deep_tsf.png"  class="text-center" style="width: 90%; max-width: 1100px">
          </div>
      </div>

      <div class="row" style="text-align:center; padding:1rem">
          <div class="col-sm-6">
            <p><h5><center>Performance evaluation of MCTS-DRL in the absence
              of obstacles with the human walking in straight line (left
              figure) and the presence of an obstacle (right figure) in
              which the robot turned left to avoid occlusion. The robot
              and human’s trajectories are shown with points and line,
              respectively. The side bar shows the time starting with the
              purple and ending with red color, respectively.</center></h5></p>
          </div>
          <div class="col-sm-6">
            <p><h5><center>Performance evaluation of MCTS-DRL in the absence
              of obstacles with the human walking in a U -shaped path
              (left figure) and the presence of an obstacle (right figure) in
              which the robot changed its direction to avoid occlusion. The
              robot and human’s orientation is shown with the green arrows
              at the time of direction altering. The robot and human’s
              trajectories are shown with points and line, respectively. The
              side bar shows the time starting with the purple and ending
              with red color, respectively.</center></h5></p>
          </div>
      </div>

      <div class="row" style="text-align:center; padding: 1rem">
          <div class="col-sm-6">
              <img src="resrc/exp3-S.png" alt="dti.png"  class="text-center" style="width: 100%; max-width: 1100px">
            </div>
            <div class="col-sm-6">
              <img src="resrc/exp3-L.png" alt="deep_tsf.png"  class="text-center" style="width: 60%; max-width: 1100px">
            </div>
      </div>


      <div class="row" style="text-align:center; padding:1rem">
            <div class="col-sm-6">
              <p><h5><center>Performance evaluation of MCTS-DRL in the absence
                  of obstacles with the human walking in a S-shaped path
                  (left figure) and the presence of an obstacle (right figure) in
                  which the robot changed its direction to avoid occlusion. The
                  robot and human’s orientation is shown with the green arrows
                  at the time of direction altering. The robot and human’s
                  trajectories are shown with points and line, respectively. The
                  side bar shows the time starting with the purple and ending
                  with red color, respectively.</center></h5></p>
            </div>
            <div class="col-sm-6">
              <p><h5><center>Trajectory of the robot and human navigating an
                  L shaped path. The trajectory of the human and robot are
                  shown with line and points, respectively. The green arrows
                  indicate the orientation of the robot and human when the
                  human walks toward the corner. The robot avoids colliding
                  with obstacles and stays beside the human subject, navigating
                  ahead of them subsequently.</center></h5></p>
            </div>
      </div>


      <h4 style="padding-top:0.5em">BibTeX</h4>
      If you find this work useful for your research, please cite:
      <div class="card">
        <div class="card-block">
          <pre class="card-text clickselect">
            @inproceedings{X,
            title={An MCTS-DRL Based Obstacle and Occlusion Avoidance Methodology in Robotic Follow-Ahead Applications},
            author={Leisiazar, Sahar AND Park, Edward AND Lim, Angelica AND Chen, Mo},   
            booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
            year={2023},
            }
          </pre>
        </div>
      </div>

</div>      








</body>
</html>
